version: "3.9"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: echo srvr | nc zookeeper 2181 || exit 1
      interval: 5s
      retries: 10

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -z localhost 9092 || exit 1
      interval: 5s
      retries: 10

# Kafka UI
  akhq:
    image: tchiotludo/akhq
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "kafka:29092"


  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      # Automatically execute tables initialization script on startup
      - ./sql/init_tables.sql:/docker-entrypoint-initdb.d/init_tables.sql
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1
      interval: 5s
      retries: 10

  # Kafka producer: loads data from CSV to Kafka
  producer:
    build: ./kafka_producer
    depends_on:
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      KAFKA_SERVER: 'kafka:29092'
      CSV_FILE_PATH: '/data/train.csv' 
    volumes:
      - ./data:/data # Mount data folder to access CSV

  # Query executor: runs SQL query and saves 'result.csv' to the 'output' folder of the host
  query_executor:
    image: clickhouse/clickhouse-server:23.8
    depends_on:
      producer:
        condition: service_completed_successfully
    volumes:
      - ./sql/query.sql:/query.sql
      - ./output:/output # Map host folder to container output to access result.csv
    entrypoint: ["/bin/bash", "-c", "cat /query.sql | clickhouse-client --host clickhouse"] # Execute query on ClickHouse
